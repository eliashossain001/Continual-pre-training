# config/cpt.yaml
# Hyperparameters for continual pretraining
model: "unsloth/mistral-7b-v0.3-bnb-4bit"
max_seq_length: 2048
load_in_4bit: true
lora_rank: 128
adapter_targets:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"
  - "embed_tokens"
  - "lm_head"
lora_alpha: 32
lora_dropout: 0.0
bias: "none"
gradient_checkpointing: "unsloth"
random_state: 3407
learning_rate: 5e-5
embedding_learning_rate: 1e-5
max_steps: 120
warmup_steps: 10
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
dataset_num_proc: 2
output_dir: "outputs/llama3-cpt"
report_to: "none"